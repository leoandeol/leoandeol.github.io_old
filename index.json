[{"authors":["admin"],"categories":null,"content":"WIP #I am a Doctoral Researcher in Machine Learning at Technische Universität Berlin. Particularily interested in general aspects of ML, I work mainly on Domain Adaptation, Semi-Supervised/Active Learning, and Anomaly Detection. I also have interests in Learning Theory and Invariances.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1657538263,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://leo.andeol.eu/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"WIP #I am a Doctoral Researcher in Machine Learning at Technische Universität Berlin. Particularily interested in general aspects of ML, I work mainly on Domain Adaptation, Semi-Supervised/Active Learning, and Anomaly Detection.","tags":null,"title":"","type":"authors"},{"authors":["léo andéol","yusei kawakami","yuichiro wada","takafumi kanamori","klaus-robert müller","grégoire montavon"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623414233,"objectID":"795e194ef5a94fcc20ae7243cb0a0ffc","permalink":"https://leo.andeol.eu/publication/andeol-2021-learning/","publishdate":"2021-06-11T12:20:57.418486Z","relpermalink":"/publication/andeol-2021-learning/","section":"publication","summary":"Domain shifts in the training data are common in practical applications of machine learning, they occur for instance when the data is coming from different sources. Ideally, a ML model should work well independently of these shifts, for example, by learning a domain-invariant representation. Moreover, privacy concerns regarding the source also require a domain-invariant representation. In this work, we provide theoretical results that link domain invariant representations -- measured by the Wasserstein distance on the joint distributions -- to a practical semi-supervised learning objective based on a cross-entropy classifier and a novel domain critic. Quantitative experiments demonstrate that the proposed approach is indeed able to practically learn such an invariant representation (between two domains), and the latter also supports models with higher predictive accuracy on both domains, comparing favorably to existing techniques.","tags":[],"title":"Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization","type":"publication"},{"authors":["yuichiro wada","shugo miyamoto","takumi nakagawa","leo andeol","wataru kumagai","takafumi kanamori"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623414233,"objectID":"cfaf071d2e154ed3a950fabc383ae7ec","permalink":"https://leo.andeol.eu/publication/wada-sedc/","publishdate":"2021-06-11T12:20:57.588094Z","relpermalink":"/publication/wada-sedc/","section":"publication","summary":"We propose a new clustering method based on a deep neural network. Given an unlabeled dataset and the number of clusters, our method directly groups the dataset into the given number of clusters in the original space. We use a conditional discrete probability distribution defined by a deep neural network as a statistical model. Our strategy is first to estimate the cluster labels of unlabeled data points selected from a high-density region, and then to conduct semi-supervised learning to train the model by using the estimated cluster labels and the remaining unlabeled data points. Lastly, by using the trained model, we obtain the estimated cluster labels of all given unlabeled data points. The advantage of our method is that it does not require key conditions. Existing clustering methods with deep neural networks assume that the cluster balance of a given dataset is uniform. Moreover, it also can be applied to various data domains as long as the data is expressed by a feature vector. In addition, it is observed that our method is robust against outliers. Therefore, the proposed method is expected to perform, on average, better than previous methods. We conducted numerical experiments on five commonly used datasets to confirm the effectiveness of the proposed method.","tags":null,"title":"Spectral Embedded Deep Clustering","type":"publication"},{"authors":["leo andeol"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623414233,"objectID":"8567874ca6b4810035227cb46a9dc722","permalink":"https://leo.andeol.eu/publication/master-thesis/","publishdate":"2021-06-11T12:20:57.719393Z","relpermalink":"/publication/master-thesis/","section":"publication","summary":"The problem of having a framework to classify graphs is becoming increasingly important in the era of data. The issue has been tackled since the beginning of the millennium and there have been significant progress. This report introduces all necessary knowledge to understand the topic and then reviews different graph kernels while focusing on a random walk kernel and its optimization. Some experiments are then conducted to verify the information given by the state of the art, and some attempts are made to accelerate the different methods to compute the kernel.","tags":null,"title":"Graph Kernels and Support Vector Machines for Pattern Recognition","type":"publication"}]